{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/bray/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 225,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#from tensorflow.contrib.tensorboard.plugins import projector\n",
    "from sklearn.manifold import TSNE\n",
    "from collections import Counter\n",
    "from six.moves import cPickle\n",
    "import numpy as np\n",
    "#import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import multiprocessing\n",
    "import os\n",
    "import sys\n",
    "import io\n",
    "import logging\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "import json\n",
    "\n",
    "from gensim.models import word2vec as w2v\n",
    "nltk.download('punkt') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [],
   "source": [
    "#with open('/Users/bray/Desktop/FOMC/FOMC.json', 'rb') as f:\n",
    "    #data = f.readlines()\n",
    "    \n",
    "#data2 = json.dumps(data)\n",
    "#print(data)\n",
    "# remove the trailing \"\\n\" from each line\n",
    "#data = '\\n'.join(map(lambda x: x.decode('utf8', 'ignore')changed_project_data['rdars']))\n",
    "\n",
    "#data = data.decode(\"utf-8\") \n",
    "\n",
    "# each element of 'data' is an individual JSON object.\n",
    "# i want to convert it into an *array* of JSON objects\n",
    "# which, in and of itself, is one large JSON object\n",
    "# basically... add square brackets to the beginning\n",
    "# and end, and have all the individual business JSON objects\n",
    "# separated by a comma\n",
    "#data_json_str = \"[\" + ','.join(data) + \"]\"\n",
    "\n",
    "# now, load it into pandas\n",
    "#data_df = pd.read_json(data)\n",
    "#print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_sentences(sentences):\n",
    "    ret = []\n",
    "    max_s = len(sentences)\n",
    "    print(\"Got \" + str(max_s) + \" sentences.\")\n",
    "    for count, s in enumerate(sentences):\n",
    "        tokens = []\n",
    "        words = re.split(r'(\\s+)', s)\n",
    "        if len(words) > 0:\n",
    "            for w in words:\n",
    "                if w is not None:\n",
    "                    w = w.strip()\n",
    "                    w = w.lower()\n",
    "                    if w.isspace() or w == \"\\n\" or w == \"\\r\":\n",
    "                        w = None\n",
    "                    if len(w) < 1:\n",
    "                        w = None\n",
    "                    if w is not None:\n",
    "                        tokens.append(w)\n",
    "        if len(tokens) > 0:\n",
    "            ret.append(tokens)\n",
    "        if count % 50 == 0:\n",
    "            print_progress(count, max_s)\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_sentences(tokens):\n",
    "    all_stopwords = load_json(\"stopwords-iso.json\")\n",
    "    extra_stopwords = [\"ssä\", \"lle\", \"h.\", \"oo\", \"on\", \"muk\", \"kov\", \"km\", \"ia\", \"täm\", \"sy\", \"but\", \":sta\", \"hi\", \"py\", \"xd\", \"rr\", \"x:\", \"smg\", \"kum\", \"uut\", \"kho\", \"k\", \"04n\", \"vtt\", \"htt\", \"väy\", \"kin\", \"#8\", \"van\", \"tii\", \"lt3\", \"g\", \"ko\", \"ett\", \"mys\", \"tnn\", \"hyv\", \"tm\", \"mit\", \"tss\", \"siit\", \"pit\", \"viel\", \"sit\", \"n\", \"saa\", \"tll\", \"eik\", \"nin\", \"nii\", \"t\", \"tmn\", \"lsn\", \"j\", \"miss\", \"pivn\", \"yhn\", \"mik\", \"tn\", \"tt\", \"sek\", \"lis\", \"mist\", \"tehd\", \"sai\", \"l\", \"thn\", \"mm\", \"k\", \"ku\", \"s\", \"hn\", \"nit\", \"s\", \"no\", \"m\", \"ky\", \"tst\", \"mut\", \"nm\", \"y\", \"lpi\", \"siin\", \"a\", \"in\", \"ehk\", \"h\", \"e\", \"piv\", \"oy\", \"p\", \"yh\", \"sill\", \"min\", \"o\", \"va\", \"el\", \"tyn\", \"na\", \"the\", \"tit\", \"to\", \"iti\", \"tehdn\", \"tlt\", \"ois\", \":\", \"v\", \"?\", \"!\", \"&\",\"//\",\"href\",\"\\\\\",\"``\"]\n",
    "    stopwords = None\n",
    "    if all_stopwords is not None:\n",
    "        stopwords = all_stopwords[\"fi\"]\n",
    "        stopwords += extra_stopwords\n",
    "    ret = []\n",
    "    max_s = len(tokens)\n",
    "    for count, sentence in enumerate(tokens):\n",
    "        if count % 50 == 0:\n",
    "            print_progress(count, max_s)\n",
    "        cleaned = []\n",
    "        for token in sentence:\n",
    "            if len(token) > 0:\n",
    "                if stopwords is not None:\n",
    "                    for s in stopwords:\n",
    "                        if token == s:\n",
    "                            token = None\n",
    "                if token is not None:\n",
    "                    if re.search(\"^[0-9\\.\\-\\s\\/]+$\", token):\n",
    "                        token = None\n",
    "                if token is not None:\n",
    "                    cleaned.append(token)\n",
    "            if len(cleaned) > 0:\n",
    "                ret.append(cleaned)\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_word_frequencies(corpus):\n",
    "    frequencies = Counter()\n",
    "    for sentence in corpus:\n",
    "        for word in sentence:\n",
    "            frequencies[word] += 1\n",
    "    freq = frequencies.most_common()\n",
    "    return freq\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [],
   "source": [
    "def try_load_or_process(filename, processor_fn, function_arg):\n",
    "    load_fn = None\n",
    "    save_fn = None\n",
    "    if filename.endswith(\"json\"):\n",
    "        load_fn = load_json\n",
    "        save_fn = save_json\n",
    "    else:\n",
    "        load_fn = load_bin\n",
    "        save_fn = save_bin\n",
    "        \n",
    "    if os.path.exists(filename):\n",
    "        return load_fn(filename)\n",
    "    else:\n",
    "        ret = processor_fn(function_arg)\n",
    "        save_fn(ret, filename)\n",
    "        return ret\n",
    "\n",
    "def print_progress(current, maximum):\n",
    "    sys.stdout.write(\"\\r\")\n",
    "    sys.stdout.flush()\n",
    "    sys.stdout.write(str(current) + \"/\" + str(maximum))\n",
    "    sys.stdout.flush()\n",
    "\n",
    "def save_bin(item, filename):\n",
    "    with open(filename, \"wb\") as f:\n",
    "        cPickle.dump(item, f)\n",
    "\n",
    "def load_bin(filename):\n",
    "    if os.path.exists(filename):\n",
    "        with open(filename, \"rb\") as f:\n",
    "            return cPickle.load(f)\n",
    "\n",
    "def save_json(variable, filename):\n",
    "    with io.open(filename, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(unicode(json.dumps(variable, indent=4, ensure_ascii=False)))\n",
    "\n",
    "def load_json(filename):\n",
    "    ret = None\n",
    "    if os.path.exists(filename):\n",
    "        try:\n",
    "            with io.open(filename, \"r\", encoding=\"utf-8\") as f:\n",
    "                ret = json.load(f)\n",
    "        except:\n",
    "            pass\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_raw_data(input_file):\n",
    "    valid = u\"0123456789abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ#@.:/ äöåÄÖÅ\"\n",
    "    url_match = \"(https?:\\/\\/[0-9a-zA-Z\\-\\_]+\\.[\\-\\_0-9a-zA-Z]+\\.?[0-9a-zA-Z\\-\\_]*\\/?.*)\"\n",
    "    name_match = \"\\@[\\_0-9a-zA-Z]+\\:?\"\n",
    "    lines = []\n",
    "    print(\"Loading raw data from: \" + input_file)\n",
    "    if os.path.exists(input_file):\n",
    "        with io.open(input_file, 'r', encoding=\"utf-8\") as f:\n",
    "            lines = f.readlines()\n",
    "    num_lines = len(lines)\n",
    "    ret = []\n",
    "    for count, text in enumerate(lines):\n",
    "        if count % 50 == 0:\n",
    "            print_progress(count, num_lines)\n",
    "        text = re.sub(url_match, u\"\", text)\n",
    "        text = re.sub(name_match, u\"\", text)\n",
    "        text = re.sub(\"\\&amp\\;?\", u\"\", text)\n",
    "        text = re.sub(\"[\\:\\.]{1,}$\", u\"\", text)\n",
    "        text = re.sub(\"^RT\\:?\", u\"\", text)\n",
    "        text = u''.join(x for x in text if x in valid)\n",
    "        text = text.strip()\n",
    "        if len(text.split()) > 5:\n",
    "            if text not in ret:\n",
    "                ret.append(text)\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_word2vec(sentences):\n",
    "    num_workers = multiprocessing.cpu_count()\n",
    "    num_features = 200\n",
    "    epoch_count = 10\n",
    "    sentence_count = len(sentences)\n",
    "    w2v_file = os.path.join(save_dir, \"word_vectors.w2v\")\n",
    "    word2vec = None\n",
    "    if os.path.exists(w2v_file):\n",
    "        print(\"w2v model loaded from \" + w2v_file)\n",
    "        word2vec = w2v.Word2Vec.load(w2v_file)\n",
    "    else:\n",
    "        word2vec = w2v.Word2Vec(sg=1,\n",
    "                                seed=1,\n",
    "                                workers=num_workers,\n",
    "                                size=num_features,\n",
    "                                min_count=6,\n",
    "                                window=5,\n",
    "                                sample=0)\n",
    "        print(\"Building vocab...\")\n",
    "        word2vec.build_vocab(sentences)\n",
    "        print(\"Word2Vec vocabulary length:\", len(word2vec.wv.vocab))\n",
    "        print(\"Training...\")\n",
    "        word2vec.train(sentences, total_examples=sentence_count, epochs=epoch_count)\n",
    "        print(\"Saving model...\")\n",
    "        word2vec.save(w2v_file)\n",
    "    return word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaning tokens\n",
      "500/528Getting word frequencies\n",
      "Unique words: 1063\n",
      "Instantiating word2vec model\n",
      "w2v model loaded from analysis/word_vectors.w2v\n",
      "word2vec vocab contains 555 items.\n",
      "word2vec items have 200 features.\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    \n",
    "    fname = 'FOMC.json'\n",
    "    with open(fname) as f:\n",
    "        content = f.readlines()\n",
    "        \n",
    "    content = [x.strip() for x in content]\n",
    "    sentence = []\n",
    "    \n",
    "    for x in content:\n",
    "        nltk_tokens = nltk.word_tokenize(x)\n",
    "        sentence.append(nltk_tokens)\n",
    "        \n",
    "        \n",
    "    print(\"Cleaning tokens\")    \n",
    "    cleaned = clean_sentences(sentence)\n",
    "    \n",
    "    print(\"Getting word frequencies\")\n",
    "    frq = get_word_frequencies(cleaned)\n",
    "    ss = len(frq)\n",
    "    print(\"Unique words: \" + str(ss))\n",
    "    \n",
    "    print(\"Instantiating word2vec model\")\n",
    "    word2vec = get_word2vec(sentence)\n",
    "    vocab = list(word2vec.wv.vocab.keys())\n",
    "    vocab_len = len(vocab)\n",
    "    print(\"word2vec vocab contains \" + str(vocab_len) + \" items.\")\n",
    "    dim01 = word2vec.wv[vocab[0]].shape[0]\n",
    "    print(\"word2vec items have \" + str(dim01) + \" features.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pprint as pp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('interest', 0.7389793395996094),\n",
      " ('paid', 0.7216812372207642),\n",
      " ('lower', 0.7159571051597595),\n",
      " ('establishment', 0.7094623446464539),\n",
      " ('rate', 0.7042255997657776),\n",
      " ('required', 0.699407696723938),\n",
      " ('primary', 0.6979073882102966),\n",
      " ('through', 0.6899096369743347),\n",
      " ('1/4', 0.6894129514694214),\n",
      " ('credit', 0.6891040205955505)]\n"
     ]
    }
   ],
   "source": [
    "pp.pprint(word2vec.wv.most_similar('rates', topn=10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
